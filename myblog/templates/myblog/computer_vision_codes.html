{% extends '_base.html' %}
{% load static %}

{% block head_title %}Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± - ÙˆØ¨Ù„Ø§Ú¯ Ø§Ù…ÛŒØ±Ù…Ø­Ù…Ø¯{% endblock %}

{% block breadcrumb_items %}
<li class="breadcrumb-item"><a href="{% url 'projects' %}">Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§</a></li>
<li class="breadcrumb-item active" aria-current="page">Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</li>
{% endblock %}

{% block content %}
<div class="container my-5">
    <div class="row">
        <div class="col-12 text-center mb-5">
            <h1 class="display-4 fw-bold text-primary mb-3">ğŸ¯ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</h1>
            <p class="lead">ğŸ“š Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¬Ø§Ù…Ø¹ Ú©Ø¯Ù‡Ø§ÛŒ OpenCVØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Python</p>
        </div>
    </div>

    <!-- Ø¨Ø®Ø´ Ù…Ø¹Ø±ÙÛŒ -->
    <div class="row mb-5">
        <div class="col-12">
            <div class="card bg-gradient-primary text-white shadow-lg">
                <div class="card-body text-center py-4">
                    <h3 class="fw-bold mb-3">ğŸš€ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±</h3>
                    <p class="mb-0">Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø² Ú©Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Ø¨Ø®Ø´ ÙÛŒÙ„ØªØ± Ùˆ Ø¬Ø³ØªØ¬Ùˆ -->
    <div class="row mb-4">
        <div class="col-12">
            <div class="card">
                <div class="card-body">
                    <div class="row align-items-center">
                        <div class="col-md-6">
                            <div class="input-group">
                                <span class="input-group-text"><i class="bi bi-search"></i></span>
                                <input type="text" class="form-control" id="searchInput" placeholder="Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø¯Ù‡Ø§...">
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="d-flex gap-2 flex-wrap">
                                <button class="btn btn-outline-primary filter-btn active" data-filter="all">Ù‡Ù…Ù‡</button>
                                <button class="btn btn-outline-primary filter-btn" data-filter="detection">ØªØ´Ø®ÛŒØµ</button>
                                <button class="btn btn-outline-primary filter-btn" data-filter="filter">ÙÛŒÙ„ØªØ±</button>
                                <button class="btn btn-outline-primary filter-btn" data-filter="tracking">Ø±Ø¯ÛŒØ§Ø¨ÛŒ</button>
                                <button class="btn btn-outline-primary filter-btn" data-filter="segmentation">Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-lg-10 mx-auto">
            <!-- Ø¨Ø®Ø´ 1: ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ -->
            <div class="card shadow-lg mb-5 code-section" data-category="detection">
                <div class="card-header bg-dark text-white d-flex justify-content-between align-items-center">
                    <h4 class="mb-0"><i class="bi bi-person-check me-2"></i>ğŸ‘¤ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ OpenCV</h4>
                    <div class="btn-group" role="group">
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="copyCode('face-detection')">
                            <i class="bi bi-clipboard"></i> Ú©Ù¾ÛŒ
                        </button>
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="downloadCode('face-detection')">
                            <i class="bi bi-download"></i> Ø¯Ø§Ù†Ù„ÙˆØ¯
                        </button>
                    </div>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù</p>
                    
                    <div class="accordion mb-3" id="faceDetectionAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingOne">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
                                    <i class="bi bi-info-circle me-2"></i> ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
                                </button>
                            </h2>
                            <div id="collapseOne" class="accordion-collapse collapse show" aria-labelledby="headingOne" data-bs-parent="#faceDetectionAccordion">
                                <div class="accordion-body">
                                    <p>Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenCV Ø§Ø³Øª Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ø¯:</p>
                                    <ul>
                                        <li>ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒ Ø¬Ù„Ùˆ Ùˆ Ù†ÛŒÙ…â€ŒØ±Ø®</li>
                                        <li>ØªØ´Ø®ÛŒØµ Ú†Ø´Ù…â€ŒÙ‡Ø§ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ú†Ù‡Ø±Ù‡</li>
                                        <li>Ø°Ø®ÛŒØ±Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ ØªØ´Ø®ÛŒØµâ€ŒÙ‡Ø§</li>
                                        <li>Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ø¢Ù…Ø§Ø±ÛŒ</li>
                                        <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <pre class="code-container p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;" id="face-detection-code"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Face Detection System
Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ OpenCV
"""

import cv2
import numpy as np
import os
import time
from datetime import datetime
import json
import threading
from queue import Queue
import logging

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedFaceDetector:
    """Ú©Ù„Ø§Ø³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ú†Ù‡Ø±Ù‡"""
    
    def __init__(self, config_file=None):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡"""
        self.config = self._load_config(config_file)
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        self.profile_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_profileface.xml'
        )
        self.eye_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_eye.xml'
        )
        self.smile_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_smile.xml'
        )
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…
        self.detection_history = []
        self.face_count = 0
        self.start_time = time.time()
        self.frame_queue = Queue(maxsize=10)
        self.result_queue = Queue()
        self.is_running = False
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ
        self.scale_factor = self.config.get('scale_factor', 1.1)
        self.min_neighbors = self.config.get('min_neighbors', 5)
        self.min_size = tuple(self.config.get('min_size', (30, 30)))
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¹Ù…ÛŒÙ‚ (DNN)
        self._load_dnn_model()
        
    def _load_config(self, config_file):
        """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„"""
        default_config = {
            'scale_factor': 1.1,
            'min_neighbors': 5,
            'min_size': [30, 30],
            'dnn_model': 'deploy.prototxt',
            'dnn_weights': 'res10_300x300_ssd_iter_140000.caffemodel',
            'confidence_threshold': 0.7
        }
        
        if config_file and os.path.exists(config_file):
            with open(config_file, 'r') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def _load_dnn_model(self):
        """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ DNN Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡"""
        try:
            model_file = self.config.get('dnn_model')
            weights_file = self.config.get('dnn_weights')
            
            if os.path.exists(model_file) and os.path.exists(weights_file):
                self.dnn_net = cv2.dnn.readNetFromCaffe(model_file, weights_file)
                self.use_dnn = True
                logger.info("Ù…Ø¯Ù„ DNN Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯")
            else:
                self.use_dnn = False
                logger.warning("ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ DNN ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø² Haar Cascade Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ DNN: {e}")
            self.use_dnn = False
    
    def detect_faces_advanced(self, frame):
        """ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        if self.use_dnn:
            return self._detect_faces_dnn(frame)
        else:
            return self._detect_faces_haar(frame)
    
    def _detect_faces_dnn(self, frame):
        """ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ DNN"""
        h, w = frame.shape[:2]
        
        # Ø§ÛŒØ¬Ø§Ø¯ blob Ø§Ø² ØªØµÙˆÛŒØ±
        blob = cv2.dnn.blobFromImage(
            frame, 1.0, (300, 300), [104, 117, 123], False, False
        )
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
        self.dnn_net.setInput(blob)
        detections = self.dnn_net.forward()
        
        faces = []
        confidences = []
        
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            
            if confidence > self.config.get('confidence_threshold', 0.7):
                x1 = int(detections[0, 0, i, 3] * w)
                y1 = int(detections[0, 0, i, 4] * h)
                x2 = int(detections[0, 0, i, 5] * w)
                y2 = int(detections[0, 0, i, 6] * h)
                
                faces.append((x1, y1, x2 - x1, y2 - y1))
                confidences.append(confidence)
        
        return faces, confidences
    
    def _detect_faces_haar(self, frame):
        """ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø¨Ø§ Haar Cascade"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù†ØªØ±Ø§Ø³Øª ØªØµÙˆÛŒØ±
        gray = cv2.equalizeHist(gray)
        
        # ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒ frontal
        faces_frontal = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=self.scale_factor,
            minNeighbors=self.min_neighbors,
            minSize=self.min_size,
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        # ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒ profile
        faces_profile = self.profile_cascade.detectMultiScale(
            gray,
            scaleFactor=self.scale_factor,
            minNeighbors=self.min_neighbors,
            minSize=self.min_size
        )
        
        # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
        all_faces = list(faces_frontal) + list(faces_profile)
        confidences = [1.0] * len(all_faces)
        
        return all_faces, confidences
    
    def analyze_facial_features(self, frame, face_rect):
        """Ø¢Ù†Ø§Ù„ÛŒØ² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù‡Ø±Ù‡"""
        x, y, w, h = face_rect
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ø­ÛŒÙ‡ Ú†Ù‡Ø±Ù‡
        face_roi = frame[y:y+h, x:x+w]
        gray_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        
        features = {
            'eyes': [],
            'smile': None,
            'glasses': False,
            'beard': False,
            'gender': None,
            'age_range': None
        }
        
        # ØªØ´Ø®ÛŒØµ Ú†Ø´Ù…â€ŒÙ‡Ø§
        eyes = self.eye_cascade.detectMultiScale(gray_roi)
        if len(eyes) >= 2:
            features['eyes'] = eyes.tolist()
        
        # ØªØ´Ø®ÛŒØµ Ù„Ø¨Ø®Ù†Ø¯
        smiles = self.smile_cascade.detectMultiScale(
            gray_roi[y+h//2:y+h, :],  # Ù†Ø§Ø­ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ú†Ù‡Ø±Ù‡
            scaleFactor=1.7,
            minNeighbors=22,
            minSize=(25, 25)
        )
        
        if len(smiles) > 0:
            features['smile'] = True
        
        # Ø¢Ù†Ø§Ù„ÛŒØ² Ø±Ù†Ú¯ Ù¾ÙˆØ³Øª (Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¹ÛŒÙ†Ú© Ùˆ Ø±ÛŒØ´)
        self._analyze_skin_features(face_roi, features)
        
        return features
    
    def _analyze_skin_features(self, face_roi, features):
        """Ø¢Ù†Ø§Ù„ÛŒØ² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ³Øª"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø±Ù†Ú¯ HSV
        hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)
        
        # ØªØ´Ø®ÛŒØµ Ø¹ÛŒÙ†Ú© (Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒÚ©ÛŒ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ú†Ø´Ù…)
        eye_region = hsv[:face_roi.shape[0]//2, :]
        avg_brightness = np.mean(eye_region[:, :, 2])
        
        if avg_brightness < 100:
            features['glasses'] = True
        
        # ØªØ´Ø®ÛŒØµ Ø±ÛŒØ´ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±Ù†Ú¯ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ú†Ù‡Ø±Ù‡)
        beard_region = hsv[3*face_roi.shape[0]//4:, :]
        avg_saturation = np.mean(beard_region[:, :, 1])
        
        if avg_saturation < 50:
            features['beard'] = True
    
    def draw_detection_info(self, frame, faces, confidences, features_list=None):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        for i, (face_rect, confidence) in enumerate(zip(faces, confidences)):
            x, y, w, h = face_rect
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ù†Ú¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø¹ØªÙ…Ø§Ø¯
            if confidence > 0.8:
                color = (0, 255, 0)  # Ø³Ø¨Ø² - Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§
            elif confidence > 0.6:
                color = (0, 255, 255)  # Ø²Ø±Ø¯ - Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…ØªÙˆØ³Ø·
            else:
                color = (0, 0, 255)  # Ù‚Ø±Ù…Ø² - Ø§Ø¹ØªÙ…Ø§Ø¯ Ù¾Ø§ÛŒÛŒÙ†
            
            # Ø±Ø³Ù… Ù…Ø³ØªØ·ÛŒÙ„ Ø¯ÙˆØ± Ú†Ù‡Ø±Ù‡
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø´Ù†Ø§Ø³Ù‡ Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯
            face_id = f"Face {i+1}"
            conf_text = f"{confidence:.2f}"
            cv2.putText(frame, face_id, (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            cv2.putText(frame, conf_text, (x, y+h+20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # Ø±Ø³Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù‡Ø±Ù‡
            if features_list and i < len(features_list):
                self._draw_facial_features(frame, face_rect, features_list[i])
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ
            self._save_detection_data(x, y, w, h, confidence)
    
    def _draw_facial_features(self, frame, face_rect, features):
        """Ø±Ø³Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù‡Ø±Ù‡"""
        x, y, w, h = face_rect
        
        # Ø±Ø³Ù… Ú†Ø´Ù…â€ŒÙ‡Ø§
        for eye in features.get('eyes', []):
            ex, ey, ew, eh = eye
            cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (255, 0, 0), 1)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù„Ø¨Ø®Ù†Ø¯
        if features.get('smile'):
            cv2.putText(frame, "Smiling :)", (x, y+h+40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¹ÛŒÙ†Ú©
        if features.get('glasses'):
            cv2.putText(frame, "Glasses", (x, y+h+60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
    
    def _save_detection_data(self, x, y, w, h, confidence):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ"""
        detection_data = {
            'timestamp': datetime.now().isoformat(),
            'position': {'x': x, 'y': y, 'width': w, 'height': h},
            'confidence': confidence,
            'face_id': self.face_count + 1
        }
        self.detection_history.append(detection_data)
        self.face_count += 1
    
    def process_frame_async(self, frame):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ… Ø¨Ù‡ ØµÙˆØ±Øª ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        if not self.frame_queue.full():
            self.frame_queue.put(frame)
        
        if not self.result_queue.empty():
            return self.result_queue.get()
        
        return None
    
    def start_async_processing(self):
        """Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._process_frames)
        self.processing_thread.start()
    
    def _process_frames(self):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¯Ø± ØªØ±Ø¯ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        while self.is_running:
            try:
                if not self.frame_queue.empty():
                    frame = self.frame_queue.get(timeout=1)
                    faces, confidences = self.detect_faces_advanced(frame)
                    self.result_queue.put((faces, confidences))
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…: {e}")
    
    def stop_async_processing(self):
        """ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = False
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()
    
    def get_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…"""
        current_time = time.time()
        runtime = current_time - self.start_time
        
        stats = {
            'total_faces_detected': self.face_count,
            'runtime_seconds': round(runtime, 2),
            'detections_per_minute': round(self.face_count / (runtime / 60), 2),
            'last_detection': self.detection_history[-1] if self.detection_history else None,
            'average_confidence': np.mean([d['confidence'] for d in self.detection_history]) if self.detection_history else 0,
            'detection_rate': len(self.detection_history) / runtime if runtime > 0 else 0
        }
        
        return stats
    
    def save_detection_report(self, filename='face_detection_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ"""
        report = {
            'system_info': {
                'version': '2.0',
                'created_at': datetime.now().isoformat(),
                'total_detections': self.face_count,
                'config': self.config
            },
            'detection_history': self.detection_history,
            'statistics': self.get_statistics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØ¡ ØªØ´Ø®ÛŒØµâ€ŒØ¯Ù‡Ù†Ø¯Ù‡
    detector = AdvancedFaceDetector()
    
    # Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
    detector.start_async_processing()
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ Ø®Ø·Ø§: Ø§Ù…Ú©Ø§Ù† Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
        return
    
    print("âœ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    print("ğŸ“ Ø±Ø§Ù‡Ù†Ù…Ø§: Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯")
    print("-" * 50)
    
    try:
        while True:
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ… Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ†
            ret, frame = cap.read()
            
            if not ret:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ… Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ†")
                break
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
            result = detector.process_frame_async(frame)
            
            if result:
                faces, confidences = result
                
                # Ø¢Ù†Ø§Ù„ÛŒØ² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù‡Ø±Ù‡
                features_list = []
                for face_rect in faces:
                    features = detector.analyze_facial_features(frame, face_rect)
                    features_list.append(features)
                
                # Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±
                detector.draw_detection_info(frame, faces, confidences, features_list)
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
            stats = detector.get_statistics()
            stats_text = [
                f"Faces Detected: {len(faces) if result else 0}",
                f"Total Faces: {stats['total_faces_detected']}",
                f"Runtime: {stats['runtime_seconds']}s",
                f"Avg Confidence: {stats['average_confidence']:.2f}"
            ]
            
            for i, text in enumerate(stats_text):
                y_position = 30 + (i * 25)
                cv2.putText(frame, text, (10, y_position),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
            cv2.imshow('Advanced Face Detection System', frame)
            
            # Ø®Ø±ÙˆØ¬ Ø¨Ø§ ÙØ´Ø§Ø± Ø¯Ø§Ø¯Ù† Ú©Ù„ÛŒØ¯ 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ… ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±...")
                break
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
    finally:
        # ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        detector.stop_async_processing()
        
        # Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹
        cap.release()
        cv2.destroyAllWindows()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        detector.save_detection_report()
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        final_stats = detector.get_statistics()
        print("\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:")
        print(f"   ğŸ‘¥ Ú©Ù„ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {final_stats['total_faces_detected']}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {final_stats['runtime_seconds']} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: {final_stats['detections_per_minute']}")
        print(f"   ğŸ¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø¹ØªÙ…Ø§Ø¯: {final_stats['average_confidence']:.2f}")
        print(f"   ğŸ“ˆ Ù†Ø±Ø® ØªØ´Ø®ÛŒØµ: {final_stats['detection_rate']:.2f} ÙØ±ÛŒÙ…/Ø«Ø§Ù†ÛŒÙ‡")
        print("=" * 50)
        print("âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®Ø§ØªÙ…Ù‡ ÛŒØ§ÙØª")

if __name__ == "__main__":
    main()
                    </code></pre>
                    
                    <!-- Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ -->
                    <div class="mt-3">
                        <div class="card">
                            <div class="card-header d-flex justify-content-between align-items-center">
                                <h6 class="mb-0">Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡</h6>
                                <button class="btn btn-sm btn-outline-secondary" onclick="toggleOutput('face-detection-output')">
                                    <i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´
                                </button>
                            </div>
                            <div class="card-body d-none" id="face-detection-output">
                                <pre class="bg-light p-3 rounded">
ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡...
==================================================
âœ… Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯
ğŸ“ Ø±Ø§Ù‡Ù†Ù…Ø§: Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯
--------------------------------------------------
ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:
   ğŸ‘¥ Ú©Ù„ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: 15
   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: 120.5 Ø«Ø§Ù†ÛŒÙ‡
   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: 7.5
   ğŸ¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø¹ØªÙ…Ø§Ø¯: 0.85
   ğŸ“ˆ Ù†Ø±Ø® ØªØ´Ø®ÛŒØµ: 0.12 ÙØ±ÛŒÙ…/Ø«Ø§Ù†ÛŒÙ‡
==================================================
âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®Ø§ØªÙ…Ù‡ ÛŒØ§ÙØª
                                </pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 2: ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ -->
            <div class="card shadow-lg mb-5 code-section" data-category="filter">
                <div class="card-header bg-info text-white d-flex justify-content-between align-items-center">
                    <h4 class="mb-0"><i class="bi bi-filter-circle me-2"></i>ğŸ¨ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡</h4>
                    <div class="btn-group" role="group">
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="copyCode('image-filters')">
                            <i class="bi bi-clipboard"></i> Ú©Ù¾ÛŒ
                        </button>
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="downloadCode('image-filters')">
                            <i class="bi bi-download"></i> Ø¯Ø§Ù†Ù„ÙˆØ¯
                        </button>
                    </div>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†ÙˆØ§Ø¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ</p>
                    
                    <div class="accordion mb-3" id="imageFiltersAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingTwo">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">
                                    <i class="bi bi-info-circle me-2"></i> ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
                                </button>
                            </h2>
                            <div id="collapseTwo" class="accordion-collapse collapse show" aria-labelledby="headingTwo" data-bs-parent="#imageFiltersAccordion">
                                <div class="accordion-body">
                                    <p>Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØµÙˆÛŒØ±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯:</p>
                                    <ul>
                                        <li>ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒ (Gaussian, Median, Bilateral)</li>
                                        <li>ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ØªØµÙˆÛŒØ± (Sharpen, Emboss)</li>
                                        <li>ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡ (Canny, Sobel)</li>
                                        <li>Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ù‡Ù†Ø±ÛŒ (Sepia, Cartoon, Sketch)</li>
                                        <li>ÙÛŒÙ„ØªØ± Ù†Ù‚Ø§Ø´ÛŒ Ø±ÙˆØºÙ†ÛŒ</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <pre class="code-container p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;" id="image-filters-code"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Image Filters and Effects
ÙÛŒÙ„ØªØ±Ù‡Ø§ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØµÙˆÛŒØ±ÛŒ
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from enum import Enum
import os
import time
import json
from datetime import datetime

class FilterType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    GAUSSIAN_BLUR = "gaussian_blur"
    MEDIAN_BLUR = "median_blur"
    BILATERAL_FILTER = "bilateral"
    SHARPEN = "sharpen"
    EDGE_DETECTION = "edge_detection"
    EMBOSS = "emboss"
    SEPIA = "sepia"
    CARTOON = "cartoon"
    SKETCH = "sketch"
    OIL_PAINTING = "oil_painting"
    VINTAGE = "vintage"
    HDR = "hdr"
    NIGHT_VISION = "night_vision"
    THERMAL = "thermal"

class AdvancedImageProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØµÙˆÛŒØ±"""
    
    def __init__(self):
        self.filter_history = []
        self.performance_stats = {}
        self.original_image = None
        self.processed_images = {}
        
    def load_image(self, image_path):
        """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªØµÙˆÛŒØ± Ø§Ø² Ù…Ø³ÛŒØ±"""
        try:
            self.original_image = cv2.imread(image_path)
            if self.original_image is None:
                raise ValueError(f"ØªØµÙˆÛŒØ± Ø¯Ø± Ù…Ø³ÛŒØ± {image_path} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return True
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªØµÙˆÛŒØ±: {e}")
            return False
    
    def save_image(self, image, filename):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ±"""
        try:
            cv2.imwrite(filename, image)
            print(f"ØªØµÙˆÛŒØ± Ø¯Ø± ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            return True
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ±: {e}")
            return False
    
    def apply_gaussian_blur(self, image, kernel_size=15, sigma=0):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ú¯ÙˆØ³ÛŒ"""
        if kernel_size % 2 == 0:
            kernel_size += 1
            
        start_time = time.time()
        blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)
        processing_time = time.time() - start_time
        
        self._record_performance("Gaussian Blur", processing_time)
        return blurred
    
    def apply_median_blur(self, image, kernel_size=15):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù…ÛŒØ§Ù†Ù‡"""
        if kernel_size % 2 == 0:
            kernel_size += 1
            
        start_time = time.time()
        blurred = cv2.medianBlur(image, kernel_size)
        processing_time = time.time() - start_time
        
        self._record_performance("Median Blur", processing_time)
        return blurred
    
    def apply_bilateral_filter(self, image, d=15, sigma_color=75, sigma_space=75):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø¯ÙˆØ·Ø±ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ù„Ø¨Ù‡â€ŒÙ‡Ø§"""
        start_time = time.time()
        filtered = cv2.bilateralFilter(image, d, sigma_color, sigma_space)
        processing_time = time.time() - start_time
        
        self._record_performance("Bilateral Filter", processing_time)
        return filtered
    
    def apply_sharpen_filter(self, image, strength=1.0):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø´Ø§Ø±Ù¾Ù†"""
        kernel = np.array([[-1, -1, -1],
                          [-1,  9, -1],
                          [-1, -1, -1]]) * strength
        start_time = time.time()
        sharpened = cv2.filter2D(image, -1, kernel)
        processing_time = time.time() - start_time
        
        self._record_performance("Sharpen", processing_time)
        return sharpened
    
    def apply_edge_detection(self, image, low_threshold=50, high_threshold=150):
        """ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Canny"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        start_time = time.time()
        edges = cv2.Canny(gray, low_threshold, high_threshold)
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        processing_time = time.time() - start_time
        
        self._record_performance("Edge Detection", processing_time)
        return edges_colored
    
    def apply_emboss_filter(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø§Ù…Ø¨Ø§Ø³ (Ø¨Ø±Ø¬Ø³ØªÙ‡)"""
        kernel = np.array([[-2, -1, 0],
                          [-1,  1, 1],
                          [ 0,  1, 2]])
        start_time = time.time()
        embossed = cv2.filter2D(image, -1, kernel)
        processing_time = time.time() - start_time
        
        self._record_performance("Emboss", processing_time)
        return embossed
    
    def apply_sepia_filter(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø³Ù¾ÛŒØ§ (Ø¹Ú©Ø³ Ù‚Ø¯ÛŒÙ…ÛŒ)"""
        sepia_filter = np.array([[0.272, 0.534, 0.131],
                                [0.349, 0.686, 0.168],
                                [0.393, 0.769, 0.189]])
        start_time = time.time()
        sepia = cv2.transform(image, sepia_filter)
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ 0-255
        sepia = np.clip(sepia, 0, 255)
        processing_time = time.time() - start_time
        
        self._record_performance("Sepia", processing_time)
        return sepia.astype(np.uint8)
    
    def apply_cartoon_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ú©Ø§Ø±ØªÙˆÙ†ÛŒ"""
        start_time = time.time()
        
        # Ú©Ø§Ù‡Ø´ Ù†ÙˆÛŒØ²
        color = cv2.bilateralFilter(image, 9, 250, 250)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù…ÛŒØ§Ù†Ù‡
        gray = cv2.medianBlur(gray, 7)
        
        # ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡
        edges = cv2.adaptiveThreshold(gray, 255, 
                                    cv2.ADAPTIVE_THRESH_MEAN_C, 
                                    cv2.THRESH_BINARY, 9, 2)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ù„Ø¨Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        
        # ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ
        cartoon = cv2.bitwise_and(color, edges)
        
        processing_time = time.time() - start_time
        self._record_performance("Cartoon", processing_time)
        return cartoon
    
    def apply_sketch_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ Ù…Ø¯Ø§Ø¯"""
        start_time = time.time()
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ù…Ø¹Ú©ÙˆØ³ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ±
        inverted = 255 - gray
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ú¯ÙˆØ³ÛŒ
        blurred = cv2.GaussianBlur(inverted, (21, 21), 0)
        
        # ØªØ±Ú©ÛŒØ¨ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø§ÙÚ©Øª Ø·Ø±Ø§Ø­ÛŒ
        sketch = cv2.divide(gray, 255 - blurred, scale=256)
        sketch_colored = cv2.cvtColor(sketch, cv2.COLOR_GRAY2BGR)
        
        processing_time = time.time() - start_time
        self._record_performance("Sketch", processing_time)
        return sketch_colored
    
    def apply_oil_painting_effect(self, image, radius=5, levels=20):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ù†Ù‚Ø§Ø´ÛŒ Ø±ÙˆØºÙ†ÛŒ"""
        start_time = time.time()
        
        h, w = image.shape[:2]
        oil_painting = np.zeros_like(image)
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ø­ÛŒÙ‡
                region = image[i-radius:i+radius+1, j-radius:j+radius+1]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø§Ù†Ø§Ù„
                intensity_counts = np.zeros(levels)
                avg_color = np.zeros(3)
                
                for x in range(region.shape[0]):
                    for y in range(region.shape[1]):
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Øª
                        intensity = int(np.mean(region[x, y]) * (levels - 1) / 255)
                        intensity_counts[intensity] += 1
                        avg_color += region[x, y]
                
                # ÛŒØ§ÙØªÙ† Ø´Ø¯Øª ØºØ§Ù„Ø¨
                dominant_intensity = np.argmax(intensity_counts)
                
                # ØªÙ†Ø¸ÛŒÙ… Ø±Ù†Ú¯ Ù¾ÛŒÚ©Ø³Ù„
                oil_painting[i, j] = avg_color / np.sum(intensity_counts)
        
        processing_time = time.time() - start_time
        self._record_performance("Oil Painting", processing_time)
        return oil_painting.astype(np.uint8)
    
    def apply_vintage_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø¹Ú©Ø³ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        start_time = time.time()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø±Ù†Ú¯ LAB
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # Ú©Ø§Ù‡Ø´ Ú©Ù†ØªØ±Ø§Ø³Øª Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ L
        l = cv2.multiply(l, 0.8)
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ù… Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ a Ùˆ b
        a = cv2.add(a, 10)
        b = cv2.add(b, 10)
        
        # ØªØ±Ú©ÛŒØ¨ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§
        lab = cv2.merge([l, a, b])
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ BGR
        vintage = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆÛŒØ²
        noise = np.random.normal(0, 15, vintage.shape).astype(np.uint8)
        vintage = cv2.add(vintage, noise)
        
        # Ú©Ø§Ù‡Ø´ ÙˆØ¶ÙˆØ­ Ù„Ø¨Ù‡â€ŒÙ‡Ø§
        vintage = cv2.GaussianBlur(vintage, (3, 3), 0)
        
        processing_time = time.time() - start_time
        self._record_performance("Vintage", processing_time)
        return vintage
    
    def apply_hdr_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª HDR (High Dynamic Range)"""
        start_time = time.time()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø±Ù†Ú¯ LAB
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # Ø§Ø¹Ù…Ø§Ù„ CLAHE (Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        # ØªØ±Ú©ÛŒØ¨ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§
        lab = cv2.merge([l, a, b])
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ BGR
        hdr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        
        processing_time = time.time() - start_time
        self._record_performance("HDR", processing_time)
        return hdr
    
    def apply_night_vision_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø¯ÛŒØ¯ Ø¯Ø± Ø´Ø¨"""
        start_time = time.time()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ú©Ù†ØªØ±Ø§Ø³Øª
        gray = cv2.equalizeHist(gray)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø±Ù†Ú¯ÛŒ Ø¨Ø§ Ø³Ø¨Ø²
        night_vision = np.zeros_like(image)
        night_vision[:, :, 1] = gray  # Ú©Ø§Ù†Ø§Ù„ Ø³Ø¨Ø²
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆÛŒØ²
        noise = np.random.normal(0, 10, gray.shape).astype(np.uint8)
        night_vision[:, :, 1] = cv2.add(night_vision[:, :, 1], noise)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø·ÙˆØ· Ø§Ø³Ú©Ù†
        for i in range(0, night_vision.shape[0], 4):
            night_vision[i:i+2, :, :] = 0
        
        processing_time = time.time() - start_time
        self._record_performance("Night Vision", processing_time)
        return night_vision
    
    def apply_thermal_effect(self, image):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙÚ©Øª Ø­Ø±Ø§Ø±ØªÛŒ"""
        start_time = time.time()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Ù…Ø¹Ú©ÙˆØ³ Ú©Ø±Ø¯Ù† (Ù…Ù†Ø§Ø·Ù‚ Ø±ÙˆØ´Ù† Ø¨Ù‡ Ø¯Ø§Øº ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯)
        gray = 255 - gray
        
        # Ø§Ø¹Ù…Ø§Ù„ colormap Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ± Ø­Ø±Ø§Ø±ØªÛŒ
        thermal = cv2.applyColorMap(gray, cv2.COLORMAP_JET)
        
        processing_time = time.time() - start_time
        self._record_performance("Thermal", processing_time)
        return thermal
    
    def create_filter_comparison(self, image, filter_types):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ† ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        fig, axes = plt.subplots(3, 4, figsize=(20, 15))
        axes = axes.ravel()
        
        # ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
        axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        axes[0].set_title('Original Image')
        axes[0].axis('off')
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ±Ù‡Ø§
        for i, filter_type in enumerate(filter_types[:11], 1):
            filtered_image = self.apply_filter(image, filter_type)
            axes[i].imshow(cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB))
            axes[i].set_title(filter_type.value.replace('_', ' ').title())
            axes[i].axis('off')
        
        plt.tight_layout()
        return fig
    
    def apply_filter(self, image, filter_type):
        """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹"""
        filter_methods = {
            FilterType.GAUSSIAN_BLUR: lambda img: self.apply_gaussian_blur(img),
            FilterType.MEDIAN_BLUR: lambda img: self.apply_median_blur(img),
            FilterType.BILATERAL_FILTER: lambda img: self.apply_bilateral_filter(img),
            FilterType.SHARPEN: lambda img: self.apply_sharpen_filter(img),
            FilterType.EDGE_DETECTION: lambda img: self.apply_edge_detection(img),
            FilterType.EMBOSS: lambda img: self.apply_emboss_filter(img),
            FilterType.SEPIA: lambda img: self.apply_sepia_filter(img),
            FilterType.CARTOON: lambda img: self.apply_cartoon_effect(img),
            FilterType.SKETCH: lambda img: self.apply_sketch_effect(img),
            FilterType.OIL_PAINTING: lambda img: self.apply_oil_painting_effect(img),
            FilterType.VINTAGE: lambda img: self.apply_vintage_effect(img),
            FilterType.HDR: lambda img: self.apply_hdr_effect(img),
            FilterType.NIGHT_VISION: lambda img: self.apply_night_vision_effect(img),
            FilterType.THERMAL: lambda img: self.apply_thermal_effect(img)
        }
        
        return filter_methods[filter_type](image)
    
    def _record_performance(self, filter_name, processing_time):
        """Ø«Ø¨Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙÛŒÙ„ØªØ±"""
        if filter_name not in self.performance_stats:
            self.performance_stats[filter_name] = []
        
        self.performance_stats[filter_name].append(processing_time)
    
    def get_performance_report(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        report = {}
        
        for filter_name, times in self.performance_stats.items():
            if times:
                report[filter_name] = {
                    'count': len(times),
                    'total_time': sum(times),
                    'average_time': sum(times) / len(times),
                    'min_time': min(times),
                    'max_time': max(times)
                }
        
        return report
    
    def save_performance_report(self, filename='filter_performance.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'performance': self.get_performance_report()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def demonstrate_filters():
    """Ù†Ù…Ø§ÛŒØ´ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
    print("ğŸ¨ Ø´Ø±ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ...")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±
    processor = AdvancedImageProcessor()
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡
    image_path = 'sample_image.jpg'
    
    if not os.path.exists(image_path):
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        print("ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡...")
        image = np.random.randint(0, 255, (400, 600, 3), dtype=np.uint8)
        cv2.imwrite(image_path, image)
    
    if not processor.load_image(image_path):
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªØµÙˆÛŒØ±")
        return
    
    image = processor.original_image
    
    # Ù„ÛŒØ³Øª ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
    filters_to_show = [
        FilterType.GAUSSIAN_BLUR,
        FilterType.MEDIAN_BLUR,
        FilterType.BILATERAL_FILTER,
        FilterType.SHARPEN,
        FilterType.EDGE_DETECTION,
        FilterType.EMBOSS,
        FilterType.SEPIA,
        FilterType.CARTOON,
        FilterType.SKETCH,
        FilterType.VINTAGE,
        FilterType.HDR,
        FilterType.NIGHT_VISION
    ]
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡
    print("ğŸ–¼ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§...")
    fig = processor.create_filter_comparison(image, filters_to_show)
    plt.savefig('filter_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ 'filter_comparison.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    vintage = processor.apply_vintage_effect(image)
    processor.save_image(vintage, 'vintage_effect.jpg')
    print("ğŸï¸ Ø§ÙÚ©Øª Ù‚Ø¯ÛŒÙ…ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'vintage_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    hdr = processor.apply_hdr_effect(image)
    processor.save_image(hdr, 'hdr_effect.jpg')
    print("ğŸŒ† Ø§ÙÚ©Øª HDR Ø¯Ø± ÙØ§ÛŒÙ„ 'hdr_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    night_vision = processor.apply_night_vision_effect(image)
    processor.save_image(night_vision, 'night_vision_effect.jpg')
    print("ğŸŒƒ Ø§ÙÚ©Øª Ø¯ÛŒØ¯ Ø¯Ø± Ø´Ø¨ Ø¯Ø± ÙØ§ÛŒÙ„ 'night_vision_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    thermal = processor.apply_thermal_effect(image)
    processor.save_image(thermal, 'thermal_effect.jpg')
    print("ğŸŒ¡ï¸ Ø§ÙÚ©Øª Ø­Ø±Ø§Ø±ØªÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'thermal_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯
    processor.save_performance_report()

if __name__ == "__main__":
    demonstrate_filters()
                    </code></pre>
                    
                    <!-- Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ -->
                    <div class="mt-3">
                        <div class="card">
                            <div class="card-header d-flex justify-content-between align-items-center">
                                <h6 class="mb-0">Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡</h6>
                                <button class="btn btn-sm btn-outline-secondary" onclick="toggleOutput('image-filters-output')">
                                    <i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´
                                </button>
                            </div>
                            <div class="card-body d-none" id="image-filters-output">
                                <pre class="bg-light p-3 rounded">
ğŸ¨ Ø´Ø±ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ...
ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡...
ğŸ–¼ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§...
âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ 'filter_comparison.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
ğŸï¸ Ø§ÙÚ©Øª Ù‚Ø¯ÛŒÙ…ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'vintage_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
ğŸŒ† Ø§ÙÚ©Øª HDR Ø¯Ø± ÙØ§ÛŒÙ„ 'hdr_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
ğŸŒƒ Ø§ÙÚ©Øª Ø¯ÛŒØ¯ Ø¯Ø± Ø´Ø¨ Ø¯Ø± ÙØ§ÛŒÙ„ 'night_vision_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
ğŸŒ¡ï¸ Ø§ÙÚ©Øª Ø­Ø±Ø§Ø±ØªÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'thermal_effect.jpg' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ filter_performance.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
                                </pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 3: Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª -->
            <div class="card shadow-lg mb-5 code-section" data-category="tracking">
                <div class="card-header bg-warning text-dark d-flex justify-content-between align-items-center">
                    <h4 class="mb-0"><i class="bi bi-activity me-2"></i>ğŸƒâ€â™‚ï¸ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯</h4>
                    <div class="btn-group" role="group">
                        <button type="button" class="btn btn-sm btn-outline-dark" onclick="copyCode('motion-detection')">
                            <i class="bi bi-clipboard"></i> Ú©Ù¾ÛŒ
                        </button>
                        <button type="button" class="btn btn-sm btn-outline-dark" onclick="downloadCode('motion-detection')">
                            <i class="bi bi-download"></i> Ø¯Ø§Ù†Ù„ÙˆØ¯
                        </button>
                    </div>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª</p>
                    
                    <div class="accordion mb-3" id="motionDetectionAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingThree">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="true" aria-controls="collapseThree">
                                    <i class="bi bi-info-circle me-2"></i> ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
                                </button>
                            </h2>
                            <div id="collapseThree" class="accordion-collapse collapse show" aria-labelledby="headingThree" data-bs-parent="#motionDetectionAccordion">
                                <div class="accordion-body">
                                    <p>Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ Ø§Ø³Øª Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ø¯:</p>
                                    <ul>
                                        <li>ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Background Subtraction</li>
                                        <li>Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ÙˆØ§Ø­ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø­Ø±Ú©Øª</li>
                                        <li>Ø«Ø¨Øª Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª</li>
                                        <li>Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ø¢Ù…Ø§Ø±ÛŒ</li>
                                        <li>Ù†Ù…Ø§ÛŒØ´ timeline ÙØ¹Ø§Ù„ÛŒØª</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <pre class="code-container p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;" id="motion-detection-code"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intelligent Motion Detection System
Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª
"""

import cv2
import numpy as np
import time
import json
from datetime import datetime, timedelta
from collections import deque
import threading
import queue
import logging
import os

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MotionAnalyzer:
    """Ø¢Ù†Ø§Ù„Ø§ÛŒØ²Ø± Ø­Ø±Ú©Øª Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø­Ø±Ú©Øª Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ"""
    
    def __init__(self, min_area=500, threshold=25, history_length=100, config_file=None):
        self.config = self._load_config(config_file)
        self.min_area = self.config.get('min_area', min_area)
        self.threshold = self.config.get('threshold', threshold)
        self.history_length = self.config.get('history_length', history_length)
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø­Ø±Ú©Øª
        self.motion_history = deque(maxlen=self.history_length)
        self.motion_events = []
        
        # background subtractor
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            history=500, 
            varThreshold=16, 
            detectShadows=True
        )
        
        # Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…
        self.motion_count = 0
        self.start_time = time.time()
        self.last_motion_time = None
        
        # ØµÙ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        self.frame_queue = queue.Queue(maxsize=10)
        self.result_queue = queue.Queue()
        self.is_running = False
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‡Ø´Ø¯Ø§Ø±
        self.alert_enabled = self.config.get('alert_enabled', True)
        self.alert_cooldown = self.config.get('alert_cooldown', 30)  # Ø«Ø§Ù†ÛŒÙ‡
        self.last_alert_time = 0
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¶Ø¨Ø·
        self.recording_enabled = self.config.get('recording_enabled', False)
        self.recording_duration = self.config.get('recording_duration', 10)  # Ø«Ø§Ù†ÛŒÙ‡
        self.video_writer = None
        self.recording_start_time = None
        
    def _load_config(self, config_file):
        """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„"""
        default_config = {
            'min_area': 500,
            'threshold': 25,
            'history_length': 100,
            'alert_enabled': True,
            'alert_cooldown': 30,
            'recording_enabled': False,
            'recording_duration': 10,
            'detection_zones': [],
            'notification_settings': {
                'email': '',
                'push_notification': False
            }
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
        
        return default_config
    
    def detect_motion(self, frame):
        """ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ø¯Ø± ÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ"""
        # Ø§Ø¹Ù…Ø§Ù„ background subtraction
        fg_mask = self.bg_subtractor.apply(frame)
        
        # Ø­Ø°Ù Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§
        _, fg_mask = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)
        
        # Ø­Ø°Ù Ù†ÙˆÛŒØ²
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)
        
        return fg_mask
    
    def analyze_motion_regions(self, frame, motion_mask):
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ÙˆØ§Ø­ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø­Ø±Ú©Øª"""
        # ÛŒØ§ÙØªÙ† Ú©Ø§Ù†ØªÙˆØ±Ù‡Ø§
        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        motion_regions = []
        motion_detected = False
        
        for contour in contours:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³Ø§Ø­Øª Ú©Ø§Ù†ØªÙˆØ±
            area = cv2.contourArea(contour)
            
            if area > self.min_area:
                motion_detected = True
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³ØªØ·ÛŒÙ„ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
                x, y, w, h = cv2.boundingRect(contour)
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ú¯Ø± Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ ØªØ´Ø®ÛŒØµ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                if self._is_in_detection_zone(x, y, w, h):
                    # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù†Ø·Ù‚Ù‡
                    region_info = {
                        'bbox': (x, y, w, h),
                        'area': area,
                        'center': (x + w//2, y + h//2),
                        'contour': contour.tolist()
                    }
                    motion_regions.append(region_info)
                    
                    # Ø±Ø³Ù… Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±
                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.putText(frame, f"Motion", (x, y-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # Ø«Ø¨Øª Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø­Ø±Ú©Øª
        if motion_detected:
            self._record_motion_event(motion_regions)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±
            if self.alert_enabled:
                self._check_alert(motion_regions)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¶Ø¨Ø·
            if self.recording_enabled:
                self._handle_recording(frame)
        
        return motion_regions, motion_detected
    
    def _is_in_detection_zone(self, x, y, w, h):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ú¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø­Ø±Ú©Øª Ø¯Ø± Ù†ÙˆØ§Ø­ÛŒ ØªØ´Ø®ÛŒØµ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯"""
        if not self.config.get('detection_zones'):
            return True  # Ø§Ú¯Ø± Ù†Ø§Ø­ÛŒÙ‡â€ŒØ§ÛŒ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ØŒ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯
        
        center_x, center_y = x + w//2, y + h//2
        
        for zone in self.config['detection_zones']:
            zone_x, zone_y, zone_w, zone_h = zone
            if (zone_x <= center_x <= zone_x + zone_w and 
                zone_y <= center_y <= zone_y + zone_h):
                return True
        
        return False
    
    def _record_motion_event(self, regions):
        """Ø«Ø¨Øª Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø­Ø±Ú©Øª"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'regions': regions,
            'region_count': len(regions),
            'total_area': sum(region['area'] for region in regions)
        }
        
        self.motion_events.append(event)
        self.motion_count += 1
        self.last_motion_time = time.time()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.motion_history.append({
            'time': time.time(),
            'motion_detected': True,
            'region_count': len(regions)
        })
    
    def _check_alert(self, regions):
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ù‡Ø´Ø¯Ø§Ø±"""
        current_time = time.time()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÙˆØ±Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ù‡Ø´Ø¯Ø§Ø±
        if current_time - self.last_alert_time < self.alert_cooldown:
            return
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Øª Ø­Ø±Ú©Øª
        total_area = sum(region['area'] for region in regions)
        
        # Ø§Ú¯Ø± Ø­Ø±Ú©Øª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ø´Ø¯Ø§Ø± Ø§Ø±Ø³Ø§Ù„ Ú©Ù†
        if total_area > self.min_area * 2:  # Ø­Ø±Ú©Øª Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² Ø­Ø¯Ø§Ù‚Ù„
            self._send_alert(regions)
            self.last_alert_time = current_time
    
    def _send_alert(self, regions):
        """Ø§Ø±Ø³Ø§Ù„ Ù‡Ø´Ø¯Ø§Ø±"""
        logger.warning(f"ğŸš¨ Ø­Ø±Ú©Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ - {len(regions)} Ù…Ù†Ø·Ù‚Ù‡")
        
        # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù‡Ø´Ø¯Ø§Ø± Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø§ÛŒÙ…ÛŒÙ„ØŒ Ù¾ÛŒØ§Ù…Ú© ÛŒØ§ Ù†ÙˆØªÛŒÙÛŒÚ©ÛŒØ´Ù† Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ± Ø§Ø² Ù„Ø­Ø¸Ù‡ Ø­Ø±Ú©Øª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        alert_image_path = f"motion_alert_{timestamp}.jpg"
        
        # Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ø¨Ø§ÛŒØ¯ Ø§Ø² ÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯
        # cv2.imwrite(alert_image_path, frame)
        
        logger.info(f"ØªØµÙˆÛŒØ± Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø± {alert_image_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def _handle_recording(self, frame):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ"""
        current_time = time.time()
        
        # Ø§Ú¯Ø± Ø¯Ø± Ø­Ø§Ù„ Ø¶Ø¨Ø· Ù†ÛŒØ³ØªÛŒÙ…ØŒ Ø´Ø±ÙˆØ¹ Ú©Ù†
        if self.video_writer is None:
            self._start_recording()
        
        # Ø§Ú¯Ø± Ø¯Ø± Ø­Ø§Ù„ Ø¶Ø¨Ø· Ù‡Ø³ØªÛŒÙ…ØŒ ÙØ±ÛŒÙ… Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
        if self.video_writer is not None:
            self.video_writer.write(frame)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ø¶Ø¨Ø·
            if current_time - self.recording_start_time >= self.recording_duration:
                self._stop_recording()
    
    def _start_recording(self):
        """Ø´Ø±ÙˆØ¹ Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        video_path = f"motion_recording_{timestamp}.avi"
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙˆÛŒØ¯ÛŒÙˆ
        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        fps = 20
        frame_size = (640, 480)  # Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        
        self.video_writer = cv2.VideoWriter(video_path, fourcc, fps, frame_size)
        self.recording_start_time = time.time()
        
        logger.info(f"Ø´Ø±ÙˆØ¹ Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ Ø¯Ø± {video_path}")
    
    def _stop_recording(self):
        """ØªÙˆÙ‚Ù Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ"""
        if self.video_writer is not None:
            self.video_writer.release()
            self.video_writer = None
            logger.info("Ø¶Ø¨Ø· ÙˆÛŒØ¯ÛŒÙˆ Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    
    def get_motion_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø­Ø±Ú©Øª"""
        current_time = time.time()
        runtime = current_time - self.start_time
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø®ÛŒØ± (5 Ø¯Ù‚ÛŒÙ‚Ù‡ Ú¯Ø°Ø´ØªÙ‡)
        recent_threshold = current_time - 300  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
        recent_events = [e for e in self.motion_history 
                        if e['time'] > recent_threshold]
        
        stats = {
            'total_motion_events': self.motion_count,
            'runtime_minutes': round(runtime / 60, 2),
            'events_per_minute': round(self.motion_count / (runtime / 60), 2),
            'recent_activity': len(recent_events),
            'last_motion_time': self.last_motion_time,
            'current_status': 'ACTIVE' if self.last_motion_time and 
                                (current_time - self.last_motion_time) < 60 else 'IDLE'
        }
        
        return stats
    
    def draw_analytics_overlay(self, frame, stats):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³ Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù†ÛŒÙ…Ù‡ Ø´ÙØ§Ù Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (300, 150), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        texts = [
            f"Motion Events: {stats['total_motion_events']}",
            f"Runtime: {stats['runtime_minutes']} min",
            f"Events/Min: {stats['events_per_minute']}",
            f"Recent Activity: {stats['recent_activity']}",
            f"Status: {stats['current_status']}"
        ]
        
        for i, text in enumerate(texts):
            y_pos = 40 + (i * 25)
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Ù†Ù…Ø§ÛŒØ´ timeline ÙØ¹Ø§Ù„ÛŒØª
        self._draw_activity_timeline(frame, stats)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØ§Ø­ÛŒ ØªØ´Ø®ÛŒØµ
        self._draw_detection_zones(frame)
        
        # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ø¶Ø¨Ø·
        if self.video_writer is not None:
            cv2.putText(frame, "REC", (frame.shape[1] - 60, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            cv2.circle(frame, (frame.shape[1] - 20, 30), 5, (0, 0, 255), -1)
    
    def _draw_activity_timeline(self, frame, stats):
        """Ø±Ø³Ù… timeline ÙØ¹Ø§Ù„ÛŒØª"""
        timeline_height = 50
        timeline_width = 200
        timeline_x = frame.shape[1] - timeline_width - 20
        timeline_y = 20
        
        # Ø±Ø³Ù… Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ timeline
        cv2.rectangle(frame, 
                     (timeline_x, timeline_y),
                     (timeline_x + timeline_width, timeline_y + timeline_height),
                     (50, 50, 50), -1)
        
        # Ø±Ø³Ù… ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±
        if self.motion_history:
            max_time = max(event['time'] for event in self.motion_history)
            min_time = min(event['time'] for event in self.motion_history)
            time_range = max_time - min_time if max_time != min_time else 1
            
            for event in self.motion_history:
                if event['motion_detected']:
                    x_pos = timeline_x + int(
                        (event['time'] - min_time) / time_range * timeline_width
                    )
                    intensity = min(event['region_count'] * 2, 10)
                    cv2.line(frame,
                            (x_pos, timeline_y),
                            (x_pos, timeline_y + timeline_height),
                            (0, 255, 0), intensity)
        
        cv2.putText(frame, "Activity Timeline", 
                   (timeline_x, timeline_y - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    def _draw_detection_zones(self, frame):
        """Ø±Ø³Ù… Ù†ÙˆØ§Ø­ÛŒ ØªØ´Ø®ÛŒØµ"""
        for zone in self.config.get('detection_zones', []):
            x, y, w, h = zone
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 2)
            cv2.putText(frame, "Detection Zone", (x, y-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
    
    def start_async_processing(self):
        """Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._process_frames)
        self.processing_thread.start()
    
    def _process_frames(self):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¯Ø± ØªØ±Ø¯ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        while self.is_running:
            try:
                if not self.frame_queue.empty():
                    frame = self.frame_queue.get(timeout=1)
                    motion_mask = self.detect_motion(frame)
                    motion_regions, motion_detected = self.analyze_motion_regions(
                        frame, motion_mask
                    )
                    self.result_queue.put((frame, motion_regions, motion_detected))
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…: {e}")
    
    def stop_async_processing(self):
        """ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = False
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()
        
        # ØªÙˆÙ‚Ù Ø¶Ø¨Ø· Ø§Ú¯Ø± Ø¯Ø± Ø­Ø§Ù„ Ø¶Ø¨Ø· Ù‡Ø³ØªÛŒÙ…
        if self.video_writer is not None:
            self._stop_recording()
    
    def save_motion_report(self, filename='motion_detection_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª"""
        report = {
            'system_info': {
                'version': '2.0',
                'created_at': datetime.now().isoformat(),
                'config': self.config
            },
            'motion_events': self.motion_events,
            'statistics': self.get_motion_statistics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

class IntelligentMotionDetectionSystem:
    """Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯"""
    
    def __init__(self, config_file=None):
        self.motion_analyzer = MotionAnalyzer(config_file=config_file)
        self.is_running = False
        self.alert_callback = None
        
    def start_monitoring(self, video_source=0, alert_callback=None):
        """Ø´Ø±ÙˆØ¹ Ù†Ø¸Ø§Ø±Øª"""
        self.alert_callback = alert_callback
        self.is_running = True
        
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯...")
        print("ğŸ“Š Ø³ÛŒØ³ØªÙ… Ø¯Ø± Ø­Ø§Ù„ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø­Ø±Ú©Øª...")
        
        # Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        self.motion_analyzer.start_async_processing()
        
        cap = cv2.VideoCapture(video_source)
        
        if not cap.isOpened():
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù…Ù†Ø¨Ø¹ ÙˆÛŒØ¯ÛŒÙˆ")
            return
        
        try:
            while self.is_running:
                ret, frame = cap.read()
                
                if not ret:
                    print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ…")
                    break
                
                # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ±
                frame = cv2.resize(frame, (640, 480))
                
                # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
                if not self.motion_analyzer.frame_queue.full():
                    self.motion_analyzer.frame_queue.put(frame.copy())
                
                # Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬
                if not self.motion_analyzer.result_queue.empty():
                    processed_frame, motion_regions, motion_detected = self.motion_analyzer.result_queue.get()
                    
                    # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±
                    stats = self.motion_analyzer.get_motion_statistics()
                    
                    # Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª
                    self.motion_analyzer.draw_analytics_overlay(processed_frame, stats)
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
                    cv2.imshow('Intelligent Motion Detection', processed_frame)
                    cv2.imshow('Motion Mask', self.motion_analyzer.detect_motion(frame))
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±
                if motion_detected and self.alert_callback:
                    self.alert_callback(motion_regions)
                
                # Ø®Ø±ÙˆØ¬ Ø¨Ø§ Ú©Ù„ÛŒØ¯ 'q'
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ… ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±...")
                    break
                    
        except KeyboardInterrupt:
            print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.is_running = False
            
            # ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
            self.motion_analyzer.stop_async_processing()
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
            final_stats = self.motion_analyzer.get_motion_statistics()
            self._print_final_report(final_stats)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
            self.motion_analyzer.save_motion_report()
    
    def _print_final_report(self, stats):
        """Ú†Ø§Ù¾ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
        print("\n" + "="*50)
        print("ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª")
        print("="*50)
        print(f"   ğŸ¯ Ú©Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª: {stats['total_motion_events']}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {stats['runtime_minutes']} Ø¯Ù‚ÛŒÙ‚Ù‡")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: {stats['events_per_minute']}")
        print(f"   ğŸ”¥ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø®ÛŒØ±: {stats['recent_activity']} Ø±ÙˆÛŒØ¯Ø§Ø¯")
        print(f"   ğŸš¦ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ: {stats['current_status']}")
        print("="*50)
    
    def stop_monitoring(self):
        """ØªÙˆÙ‚Ù Ù†Ø¸Ø§Ø±Øª"""
        self.is_running = False

def motion_alert_handler(motion_regions):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª"""
    print(f"ğŸš¨ Ù‡Ø´Ø¯Ø§Ø±! Ø­Ø±Ú©Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ - {len(motion_regions)} Ù…Ù†Ø·Ù‚Ù‡")
    
    for i, region in enumerate(motion_regions):
        print(f"   ğŸ“ Ù…Ù†Ø·Ù‚Ù‡ {i+1}: Ù…ÙˆÙ‚Ø¹ÛŒØª {region['center']}, Ù…Ø³Ø§Ø­Øª {region['area']}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    print("ğŸƒâ€â™‚ï¸ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯")
    print("ğŸ“ Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ø­Ø±Ú©Øª Ø±Ø§ Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ ØªØ´Ø®ÛŒØµ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯")
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    config_file = "motion_detection_config.json"
    if not os.path.exists(config_file):
        config = {
            "min_area": 500,
            "threshold": 25,
            "history_length": 100,
            "alert_enabled": True,
            "alert_cooldown": 30,
            "recording_enabled": False,
            "recording_duration": 10,
            "detection_zones": [
                [100, 100, 200, 200],  # Ù†Ø§Ø­ÛŒÙ‡ ØªØ´Ø®ÛŒØµ Ø§ÙˆÙ„
                [400, 300, 150, 150]   # Ù†Ø§Ø­ÛŒÙ‡ ØªØ´Ø®ÛŒØµ Ø¯ÙˆÙ…
            ]
        }
        
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"ğŸ“ ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± {config_file} Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    system = IntelligentMotionDetectionSystem(config_file=config_file)
    
    # Ø´Ø±ÙˆØ¹ Ù†Ø¸Ø§Ø±Øª Ø¨Ø§ callback Ù‡Ø´Ø¯Ø§Ø±
    system.start_monitoring(
        video_source=0,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ† Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        alert_callback=motion_alert_handler
    )

if __name__ == "__main__":
    main()
                    </code></pre>
                    
                    <!-- Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ -->
                    <div class="mt-3">
                        <div class="card">
                            <div class="card-header d-flex justify-content-between align-items-center">
                                <h6 class="mb-0">Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡</h6>
                                <button class="btn btn-sm btn-outline-secondary" onclick="toggleOutput('motion-detection-output')">
                                    <i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´
                                </button>
                            </div>
                            <div class="card-body d-none" id="motion-detection-output">
                                <pre class="bg-light p-3 rounded">
ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯...
ğŸ“Š Ø³ÛŒØ³ØªÙ… Ø¯Ø± Ø­Ø§Ù„ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø­Ø±Ú©Øª...
ğŸ“ ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± motion_detection_config.json Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯
ğŸš¨ Ù‡Ø´Ø¯Ø§Ø±! Ø­Ø±Ú©Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ - 2 Ù…Ù†Ø·Ù‚Ù‡
   ğŸ“ Ù…Ù†Ø·Ù‚Ù‡ 1: Ù…ÙˆÙ‚Ø¹ÛŒØª (320, 240), Ù…Ø³Ø§Ø­Øª 1250
   ğŸ“ Ù…Ù†Ø·Ù‚Ù‡ 2: Ù…ÙˆÙ‚Ø¹ÛŒØª (480, 360), Ù…Ø³Ø§Ø­Øª 850

==================================================
ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª
==================================================
   ğŸ¯ Ú©Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø­Ø±Ú©Øª: 12
   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: 5.25 Ø¯Ù‚ÛŒÙ‚Ù‡
   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡: 2.29
   ğŸ”¥ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø®ÛŒØ±: 3 Ø±ÙˆÛŒØ¯Ø§Ø¯
   ğŸš¦ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ: IDLE
==================================================
                                </pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 4: Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO -->
            <div class="card shadow-lg mb-5 code-section" data-category="detection">
                <div class="card-header bg-success text-white d-flex justify-content-between align-items-center">
                    <h4 class="mb-0"><i class="bi bi-bounding-box me-2"></i>ğŸ“¦ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO</h4>
                    <div class="btn-group" role="group">
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="copyCode('yolo-detection')">
                            <i class="bi bi-clipboard"></i> Ú©Ù¾ÛŒ
                        </button>
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="downloadCode('yolo-detection')">
                            <i class="bi bi-download"></i> Ø¯Ø§Ù†Ù„ÙˆØ¯
                        </button>
                    </div>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ YOLO</p>
                    
                    <div class="accordion mb-3" id="yoloDetectionAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingFour">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="true" aria-controls="collapseFour">
                                    <i class="bi bi-info-circle me-2"></i> ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
                                </button>
                            </h2>
                            <div id="collapseFour" class="accordion-collapse collapse show" aria-labelledby="headingFour" data-bs-parent="#yoloDetectionAccordion">
                                <div class="accordion-body">
                                    <p>Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ YOLO Ø§Ø³Øª Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ø¯:</p>
                                    <ul>
                                        <li>ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² YOLOv3</li>
                                        <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ</li>
                                        <li>Ù†Ù…Ø§ÛŒØ´ Ø§Ø¹ØªÙ…Ø§Ø¯ ØªØ´Ø®ÛŒØµ</li>
                                        <li>Ø¢Ù…Ø§Ø± Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² ØªØ´Ø®ÛŒØµ</li>
                                        <li>Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ú©Ø§Ù…Ù„</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <pre class="code-container p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;" id="yolo-detection-code"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Real-time Object Detection with YOLO
Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ YOLO
"""

import cv2
import numpy as np
import time
import requests
import json
from datetime import datetime
import os
import threading
import queue
import logging

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class YOLODetector:
    """Ú©Ù„Ø§Ø³ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO"""
    
    def __init__(self, config_path, weights_path, classes_path, confidence_threshold=0.5, nms_threshold=0.4):
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ YOLO
        try:
            self.net = cv2.dnn.readNetFromDarknet(config_path, weights_path)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            self.model_loaded = True
            logger.info("Ù…Ø¯Ù„ YOLO Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ YOLO: {e}")
            self.model_loaded = False
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        try:
            with open(classes_path, 'r') as f:
                self.classes = [line.strip() for line in f.readlines()]
            logger.info(f"{len(self.classes)} Ú©Ù„Ø§Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ÙØ§ÛŒÙ„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {e}")
            self.classes = []
        
        # Ú¯Ø±ÙØªÙ† Ù†Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        if self.model_loaded:
            layer_names = self.net.getLayerNames()
            self.output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]
        
        # Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))
        
        # Ø¢Ù…Ø§Ø± ØªØ´Ø®ÛŒØµ
        self.detection_history = []
        self.frame_count = 0
        self.start_time = time.time()
        
        # ØµÙ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        self.frame_queue = queue.Queue(maxsize=5)
        self.result_queue = queue.Queue()
        self.is_running = False
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.target_size = (416, 416)
        self.processing_times = []
    
    def detect_objects(self, frame):
        """ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…"""
        if not self.model_loaded:
            return [], 0
        
        self.frame_count += 1
        
        height, width = frame.shape[:2]
        
        # Ø§ÛŒØ¬Ø§Ø¯ blob Ø§Ø² ØªØµÙˆÛŒØ±
        blob = cv2.dnn.blobFromImage(frame, 1/255.0, self.target_size, swapRB=True, crop=False)
        self.net.setInput(blob)
        
        # Ø§Ø¬Ø±Ø§ÛŒ forward pass
        start_time = time.time()
        layer_outputs = self.net.forward(self.output_layers)
        inference_time = time.time() - start_time
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.processing_times.append(inference_time)
        if len(self.processing_times) > 100:
            self.processing_times.pop(0)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
        boxes = []
        confidences = []
        class_ids = []
        
        for output in layer_outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                
                if confidence > self.confidence_threshold:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø®ØªØµØ§Øª Ø¬Ø¹Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    
                    # Ù…Ø®ØªØµØ§Øª Ú¯ÙˆØ´Ù‡ Ø¨Ø§Ù„Ø§-Ú†Ù¾
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        
        # Ø§Ø¹Ù…Ø§Ù„ Non-Maximum Suppression
        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confidence_threshold, self.nms_threshold)
        
        detected_objects = []
        
        if len(indices) > 0:
            for i in indices.flatten():
                x, y, w, h = boxes[i]
                confidence = confidences[i]
                class_id = class_ids[i]
                
                detected_objects.append({
                    'class_id': class_id,
                    'class_name': self.classes[class_id] if class_id < len(self.classes) else 'unknown',
                    'confidence': confidence,
                    'bbox': (x, y, w, h),
                    'center': (x + w//2, y + h//2)
                })
        
        # Ø«Ø¨Øª Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self._record_detection(detected_objects, inference_time)
        
        return detected_objects, inference_time
    
    def _record_detection(self, detected_objects, inference_time):
        """Ø«Ø¨Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ´Ø®ÛŒØµ"""
        detection_data = {
            'timestamp': datetime.now().isoformat(),
            'frame_number': self.frame_count,
            'objects_detected': len(detected_objects),
            'inference_time': inference_time,
            'objects': detected_objects
        }
        
        self.detection_history.append(detection_data)
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡
        if len(self.detection_history) > 1000:
            self.detection_history.pop(0)
    
    def draw_detections(self, frame, detected_objects):
        """Ø±Ø³Ù… Ø¬Ø¹Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±"""
        for obj in detected_objects:
            x, y, w, h = obj['bbox']
            class_name = obj['class_name']
            confidence = obj['confidence']
            class_id = obj['class_id']
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ù†Ú¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³
            if class_id < len(self.colors):
                color = self.colors[class_id].astype(int).tolist()
            else:
                color = (0, 255, 0)  # Ø³Ø¨Ø² Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ù†Ú¯ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            
            # Ø±Ø³Ù… Ø¬Ø¹Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†Ù†Ø¯Ù‡
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # Ø±Ø³Ù… Ø¨Ø±Ú†Ø³Ø¨
            label = f"{class_name}: {confidence:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            
            # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ú†Ø³Ø¨
            cv2.rectangle(frame, (x, y - label_size[1] - 10), 
                         (x + label_size[0], y), color, -1)
            
            # Ù…ØªÙ† Ø¨Ø±Ú†Ø³Ø¨
            cv2.putText(frame, label, (x, y - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # Ø±Ø³Ù… Ù…Ø±Ú©Ø² Ø´ÛŒØ¡
            center_x, center_y = obj['center']
            cv2.circle(frame, (center_x, center_y), 3, color, -1)
    
    def draw_analytics(self, frame, detected_objects, inference_time):
        """Ø±Ø³Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³"""
        # Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù†ÛŒÙ…Ù‡ Ø´ÙØ§Ù
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # Ø¢Ù…Ø§Ø± Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
        current_time = time.time()
        runtime = current_time - self.start_time
        fps = self.frame_count / runtime if runtime > 0 else 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
        avg_inference_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0
        
        texts = [
            f"Objects Detected: {len(detected_objects)}",
            f"Inference Time: {inference_time*1000:.1f}ms",
            f"Avg Inference: {avg_inference_time*1000:.1f}ms",
            f"FPS: {fps:.1f}",
            f"Frame: {self.frame_count}",
            f"Runtime: {runtime:.1f}s"
        ]
        
        for i, text in enumerate(texts):
            y_pos = 35 + (i * 20)
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        class_counts = {}
        for obj in detected_objects:
            class_name = obj['class_name']
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        y_pos = 140
        for class_name, count in list(class_counts.items())[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 Ú©Ù„Ø§Ø³ Ø§ÙˆÙ„
            text = f"{class_name}: {count}"
            cv2.putText(frame, text, (20, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            y_pos += 15
    
    def get_detection_statistics(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± ØªØ´Ø®ÛŒØµ"""
        total_objects = sum(len(detection['objects']) for detection in self.detection_history)
        avg_inference_time = np.mean([detection['inference_time'] 
                                    for detection in self.detection_history[-100:]])  # 100 ÙØ±ÛŒÙ… Ø¢Ø®Ø±
        
        stats = {
            'total_frames': self.frame_count,
            'total_objects_detected': total_objects,
            'average_inference_time': avg_inference_time,
            'objects_per_frame': total_objects / self.frame_count if self.frame_count > 0 else 0,
            'runtime': time.time() - self.start_time,
            'fps': self.frame_count / (time.time() - self.start_time) if time.time() - self.start_time > 0 else 0
        }
        
        return stats
    
    def save_detection_report(self, filename='yolo_detection_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ"""
        report = {
            'system_info': {
                'model': 'YOLO Object Detection',
                'confidence_threshold': self.confidence_threshold,
                'nms_threshold': self.nms_threshold,
                'total_frames_processed': self.frame_count,
                'report_generated': datetime.now().isoformat()
            },
            'detection_history': self.detection_history[-1000:],  # 1000 ØªØ´Ø®ÛŒØµ Ø¢Ø®Ø±
            'statistics': self.get_detection_statistics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def start_async_processing(self):
        """Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._process_frames)
        self.processing_thread.start()
    
    def _process_frames(self):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¯Ø± ØªØ±Ø¯ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        while self.is_running:
            try:
                if not self.frame_queue.empty():
                    frame = self.frame_queue.get(timeout=1)
                    detected_objects, inference_time = self.detect_objects(frame)
                    self.result_queue.put((frame, detected_objects, inference_time))
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…: {e}")
    
    def stop_async_processing(self):
        """ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†"""
        self.is_running = False
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()

def download_yolo_files():
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² YOLO"""
    base_url = "https://github.com/pjreddie/darknet/raw/master/"
    
    files = {
        "yolov3.cfg": f"{base_url}cfg/yolov3.cfg",
        "yolov3.weights": "https://pjreddie.com/media/files/yolov3.weights",
        "coco.names": f"{base_url}data/coco.names"
    }
    
    for filename, url in files.items():
        if not os.path.exists(filename):
            print(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {filename}...")
            try:
                response = requests.get(url, stream=True)
                response.raise_for_status()
                
                with open(filename, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"âœ… {filename} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯")
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {filename}: {e}")
                return False
    
    return True

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO...")
    print("ğŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„...")
    
    # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ YOLO
    config_path = 'yolov3.cfg'
    weights_path = 'yolov3.weights'
    classes_path = 'coco.names'
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
    if not all(os.path.exists(path) for path in [config_path, weights_path, classes_path]):
        print("ğŸ“¥ Ø¨Ø±Ø®ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ YOLO ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯...")
        if not download_yolo_files():
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ YOLO")
            return
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØ´Ø®ÛŒØµâ€ŒØ¯Ù‡Ù†Ø¯Ù‡
        detector = YOLODetector(config_path, weights_path, classes_path)
        
        if not detector.model_loaded:
            print("âŒ Ù…Ø¯Ù„ YOLO Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø´Ø¯")
            return
        
        # Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        detector.start_async_processing()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ±Ø¨ÛŒÙ†
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ†")
            return
        
        print("ğŸ“¹ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        print("ğŸ¯ Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡... (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯)")
        
        while True:
            ret, frame = cap.read()
            
            if not ret:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ…")
                break
            
            # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ±
            frame = cv2.resize(frame, (640, 480))
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
            if not detector.frame_queue.full():
                detector.frame_queue.put(frame.copy())
            
            # Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬
            if not detector.result_queue.empty():
                processed_frame, detected_objects, inference_time = detector.result_queue.get()
                
                # Ø±Ø³Ù… Ù†ØªØ§ÛŒØ¬
                detector.draw_detections(processed_frame, detected_objects)
                detector.draw_analytics(processed_frame, detected_objects, inference_time)
                
                # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
                cv2.imshow('YOLO Object Detection', processed_frame)
            
            # Ø®Ø±ÙˆØ¬ Ø¨Ø§ Ú©Ù„ÛŒØ¯ 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\nğŸ›‘ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ…...")
                break
                
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
    finally:
        # ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
        detector.stop_async_processing()
        
        # Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹
        if 'cap' in locals():
            cap.release()
        cv2.destroyAllWindows()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        detector.save_detection_report()
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        final_stats = detector.get_detection_statistics()
        print("\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:")
        print(f"   ğŸ¯ Ú©Ù„ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {final_stats['total_frames']}")
        print(f"   ğŸ“¦ Ú©Ù„ Ø§Ø´ÛŒØ§Ø¡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {final_stats['total_objects_detected']}")
        print(f"   âš¡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† inference: {final_stats['average_inference_time']*1000:.1f}ms")
        print(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…: {final_stats['objects_per_frame']:.2f}")
        print(f"   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {final_stats['runtime']:.1f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"   ğŸš€ FPS: {final_stats['fps']:.1f}")

if __name__ == "__main__":
    main()
                    </code></pre>
                    
                    <!-- Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ -->
                    <div class="mt-3">
                        <div class="card">
                            <div class="card-header d-flex justify-content-between align-items-center">
                                <h6 class="mb-0">Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡</h6>
                                <button class="btn btn-sm btn-outline-secondary" onclick="toggleOutput('yolo-detection-output')">
                                    <i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´
                                </button>
                            </div>
                            <div class="card-body d-none" id="yolo-detection-output">
                                <pre class="bg-light p-3 rounded">
ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø§ YOLO...
ğŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„...
80 Ú©Ù„Ø§Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯
Ù…Ø¯Ù„ YOLO Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯
ğŸ“¹ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯
ğŸ¯ Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡... (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯)

ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:
   ğŸ¯ Ú©Ù„ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: 245
   ğŸ“¦ Ú©Ù„ Ø§Ø´ÛŒØ§Ø¡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: 532
   âš¡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† inference: 125.3ms
   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…: 2.17
   â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: 45.2 Ø«Ø§Ù†ÛŒÙ‡
   ğŸš€ FPS: 5.4
                                </pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Ø¨Ø®Ø´ 5: Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± -->
            <div class="card shadow-lg mb-5 code-section" data-category="segmentation">
                <div class="card-header bg-danger text-white d-flex justify-content-between align-items-center">
                    <h4 class="mb-0"><i class="bi bi-layers me-2"></i>ğŸ¨ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab</h4>
                    <div class="btn-group" role="group">
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="copyCode('image-segmentation')">
                            <i class="bi bi-clipboard"></i> Ú©Ù¾ÛŒ
                        </button>
                        <button type="button" class="btn btn-sm btn-outline-light" onclick="downloadCode('image-segmentation')">
                            <i class="bi bi-download"></i> Ø¯Ø§Ù†Ù„ÙˆØ¯
                        </button>
                    </div>
                </div>
                <div class="card-body">
                    <p class="text-muted mb-4">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ DeepLab</p>
                    
                    <div class="accordion mb-3" id="imageSegmentationAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingFive">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFive" aria-expanded="true" aria-controls="collapseFive">
                                    <i class="bi bi-info-circle me-2"></i> ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
                                </button>
                            </h2>
                            <div id="collapseFive" class="accordion-collapse collapse show" aria-labelledby="headingFive" data-bs-parent="#imageSegmentationAccordion">
                                <div class="accordion-body">
                                    <p>Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ DeepLab Ø§Ø³Øª Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ø¯:</p>
                                    <ul>
                                        <li>Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ ØªØµÙˆÛŒØ±</li>
                                        <li>Ø±Ù†Ú¯â€ŒØ¢Ù…ÛŒØ²ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù</li>
                                        <li>Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ØªØ§ÛŒØ¬ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ</li>
                                        <li>Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø´ÛŒØ§Ø¡ Ø®Ø§Øµ</li>
                                        <li>Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø´ÛŒØ§Ø¡</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <pre class="code-container p-4 rounded" style="direction: ltr; text-align: left; max-height: 400px; overflow-y: auto;" id="image-segmentation-code"><code class="language-python">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Image Segmentation with DeepLab
Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab
"""

import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict
import time
import json
from datetime import datetime
import os
import logging

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeepLabSegmentation:
    """Ú©Ù„Ø§Ø³ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab"""
    
    def __init__(self, model_name='deeplabv3_resnet101', device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.model_name = model_name
        self.model = None
        self.preprocess = None
        self.class_names = self._get_class_names()
        self.colors = self._generate_colors()
        
        # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„
        self._load_model()
        
        # Ø¢Ù…Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.processing_times = []
    
    def _get_class_names(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Pascal VOC"""
        return [
            'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
            'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
            'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',
            'train', 'tvmonitor'
        ]
    
    def _generate_colors(self):
        """ØªÙˆÙ„ÛŒØ¯ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³"""
        np.random.seed(42)
        colors = []
        for _ in range(len(self.class_names)):
            color = (np.random.randint(0, 255), 
                    np.random.randint(0, 255), 
                    np.random.randint(0, 255))
            colors.append(color)
        return colors
    
    def _load_model(self):
        """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ DeepLab"""
        try:
            logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ {self.model_name}...")
            
            if self.model_name == 'deeplabv3_resnet101':
                self.model = torch.hub.load('pytorch/vision:v0.10.0', 
                                          'deeplabv3_resnet101', 
                                          pretrained=True)
            elif self.model_name == 'deeplabv3_mobilenet':
                self.model = torch.hub.load('pytorch/vision:v0.10.0', 
                                          'deeplabv3_mobilenet_v3_large', 
                                          pretrained=True)
            else:
                logger.error(f"Ù…Ø¯Ù„ {self.model_name} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")
                return
            
            self.model.to(self.device)
            self.model.eval()
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
            self.preprocess = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225]),
            ])
            
            logger.info(f"âœ… Ù…Ø¯Ù„ {self.model_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„: {e}")
            self.model = None
    
    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ PIL Image
        if isinstance(image, np.ndarray):
            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        input_tensor = self.preprocess(image)
        input_batch = input_tensor.unsqueeze(0)  # Ø§ÛŒØ¬Ø§Ø¯ batch dimension
        
        return input_batch.to(self.device)
    
    def segment_image(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±"""
        if self.model is None:
            raise ValueError("Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
        
        start_time = time.time()
        
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        input_batch = self.preprocess_image(image)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
        with torch.no_grad():
            output = self.model(input_batch)['out'][0]
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ numpy
        output_predictions = output.argmax(0).cpu().numpy()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú© Ø±Ù†Ú¯ÛŒ
        colored_mask = self._create_colored_mask(output_predictions)
        
        # Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ØªØ§ÛŒØ¬
        analysis = self._analyze_segmentation(output_predictions)
        
        inference_time = time.time() - start_time
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.processing_times.append(inference_time)
        if len(self.processing_times) > 100:
            self.processing_times.pop(0)
        
        return colored_mask, {
            'mask': output_predictions,
            'analysis': analysis,
            'inference_time': inference_time
        }
    
    def _create_colored_mask(self, mask: np.ndarray) -> np.ndarray:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú© Ø±Ù†Ú¯ÛŒ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ"""
        h, w = mask.shape
        colored_mask = np.zeros((h, w, 3), dtype=np.uint8)
        
        for class_id in range(len(self.class_names)):
            colored_mask[mask == class_id] = self.colors[class_id]
        
        return colored_mask
    
    def _analyze_segmentation(self, mask: np.ndarray) -> Dict:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ù†ØªØ§ÛŒØ¬ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ"""
        unique, counts = np.unique(mask, return_counts=True)
        total_pixels = mask.size
        
        class_distribution = {}
        for class_id, count in zip(unique, counts):
            class_name = self.class_names[class_id]
            percentage = (count / total_pixels) * 100
            class_distribution[class_name] = {
                'pixel_count': int(count),
                'percentage': float(percentage),
                'color': self.colors[class_id]
            }
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        main_classes = sorted(class_distribution.items(), 
                            key=lambda x: x[1]['percentage'], 
                            reverse=True)[:5]
        
        return {
            'class_distribution': class_distribution,
            'main_classes': main_classes,
            'total_classes_detected': len(unique),
            'dominant_class': main_classes[0][0] if main_classes else None
        }
    
    def overlay_mask(self, original_image: np.ndarray, mask: np.ndarray, 
                    alpha: float = 0.5) -> np.ndarray:
        """ØªØ±Ú©ÛŒØ¨ Ù…Ø§Ø³Ú© Ø¨Ø§ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ"""
        # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ø§Ø³Ú© Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
        if mask.shape[:2] != original_image.shape[:2]:
            mask = cv2.resize(mask, (original_image.shape[1], original_image.shape[0]))
        
        # ØªØ±Ú©ÛŒØ¨ ØªØµØ§ÙˆÛŒØ±
        overlay = cv2.addWeighted(original_image, 1 - alpha, mask, alpha, 0)
        
        return overlay
    
    def create_segmentation_visualization(self, original_image: np.ndarray, 
                                        mask: np.ndarray, 
                                        analysis: Dict) -> np.ndarray:
        """Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ú©Ø§Ù…Ù„ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡"""
        h, w = original_image.shape[:2]
        
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± ØªØ±Ú©ÛŒØ¨ÛŒ
        overlay = self.overlay_mask(original_image, mask, 0.6)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        y_offset = 30
        for class_name, info in analysis['main_classes']:
            color = info['color']
            percentage = info['percentage']
            
            # Ø±Ø³Ù… Ù…Ø³ØªØ·ÛŒÙ„ Ø±Ù†Ú¯ÛŒ
            cv2.rectangle(overlay, (10, y_offset - 20), (40, y_offset), color, -1)
            
            # Ù…ØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            text = f"{class_name}: {percentage:.1f}%"
            cv2.putText(overlay, text, (50, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            y_offset += 30
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù†
        cv2.putText(overlay, "Semantic Segmentation", (10, h - 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        return overlay
    
    def extract_object_masks(self, mask: np.ndarray, 
                           target_classes: List[str]) -> Dict[str, np.ndarray]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø§Ø³Ú© Ø§Ø´ÛŒØ§Ø¡ Ø®Ø§Øµ"""
        object_masks = {}
        
        for class_name in target_classes:
            if class_name in self.class_names:
                class_id = self.class_names.index(class_name)
                object_mask = (mask == class_id).astype(np.uint8) * 255
                object_masks[class_name] = object_mask
        
        return object_masks
    
    def measure_object_properties(self, original_image: np.ndarray, 
                                mask: np.ndarray) -> Dict[str, Dict]:
        """Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø´ÛŒØ§Ø¡"""
        properties = {}
        
        for class_id in np.unique(mask):
            if class_id == 0:  # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
                continue
            
            class_name = self.class_names[class_id]
            class_mask = (mask == class_id).astype(np.uint8)
            
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ø§Ù†ØªÙˆØ±Ù‡Ø§
            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, 
                                         cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ú©Ø§Ù†ØªÙˆØ±
                largest_contour = max(contours, key=cv2.contourArea)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
                area = cv2.contourArea(largest_contour)
                perimeter = cv2.arcLength(largest_contour, True)
                
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter ** 2)
                else:
                    circularity = 0
                
                # Ù…Ø³ØªØ·ÛŒÙ„ Ù…Ø­Ø§Ø·ÛŒ
                x, y, w, h = cv2.boundingRect(largest_contour)
                
                # Ù„Ø­Ø¸Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±
                moments = cv2.moments(largest_contour)
                if moments['m00'] != 0:
                    cx = int(moments['m10'] / moments['m00'])
                    cy = int(moments['m01'] / moments['m00'])
                else:
                    cx, cy = x + w // 2, y + h // 2
                
                properties[class_name] = {
                    'area': float(area),
                    'perimeter': float(perimeter),
                    'circularity': float(circularity),
                    'bounding_box': (x, y, w, h),
                    'centroid': (cx, cy),
                    'aspect_ratio': float(w / h) if h > 0 else 0
                }
        
        return properties
    
    def real_time_segmentation(self, camera_index=0):
        """Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ†"""
        cap = cv2.VideoCapture(camera_index)
        
        if not cap.isOpened():
            logger.error("Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ†")
            return
        
        logger.info("ğŸ¥ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ...")
        logger.info("Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯")
        
        try:
            while True:
                ret, frame = cap.read()
                
                if not ret:
                    break
                
                # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ±
                frame = cv2.resize(frame, (640, 480))
                
                # Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±
                try:
                    colored_mask, results = self.segment_image(frame)
                    
                    # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù‡Ø§ÛŒÛŒ
                    visualization = self.create_segmentation_visualization(
                        frame, colored_mask, results['analysis']
                    )
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ø²Ù…Ø§Ù† inference
                    avg_inference_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0
                    cv2.putText(visualization, 
                               f"Avg Inference: {avg_inference_time*1000:.1f}ms",
                               (10, visualization.shape[0] - 50),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ: {e}")
                    visualization = frame
                
                cv2.imshow('Real-time Segmentation', visualization)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ ØªÙˆÙ‚Ù ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±")
        finally:
            cap.release()
            cv2.destroyAllWindows()
    
    def save_segmentation_report(self, results: Dict, filename='segmentation_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'model': self.model_name,
            'device': self.device,
            'results': results,
            'performance': {
                'average_inference_time': sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0,
                'total_processed': len(self.processing_times)
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    print("ğŸ¨ Ø³ÛŒØ³ØªÙ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯
    segmenter = DeepLabSegmentation()
    
    if segmenter.model is None:
        print("âŒ Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø´Ø¯ØŒ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡")
        return
    
    # Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡
    image_path = "sample_image.jpg"
    
    try:
        # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØµÙˆÛŒØ±
        if os.path.exists(image_path):
            image = cv2.imread(image_path)
        else:
            # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡
            print("ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡...")
            image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            cv2.imwrite(image_path, image)
        
        # Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ
        print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±...")
        colored_mask, results = segmenter.segment_image(image)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù‡Ø§ÛŒÛŒ
        visualization = segmenter.create_segmentation_visualization(
            image, colored_mask, results['analysis']
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        cv2.imwrite("segmentation_result.jpg", visualization)
        cv2.imwrite("segmentation_mask.jpg", colored_mask)
        
        print("âœ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        print(f"â±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: {results['inference_time']*1000:.1f} Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡")
        print(f"ğŸ¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {results['analysis']['total_classes_detected']}")
        print(f"ğŸ† Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ: {results['analysis']['dominant_class']}")
        
        # Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        properties = segmenter.measure_object_properties(image, results['mask'])
        print("\nğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø´ÛŒØ§Ø¡:")
        for class_name, props in properties.items():
            print(f"   {class_name}:")
            print(f"      Ù…Ø³Ø§Ø­Øª: {props['area']:.0f} Ù¾ÛŒÚ©Ø³Ù„")
            print(f"      Ù…Ø­ÛŒØ·: {props['perimeter']:.0f} Ù¾ÛŒÚ©Ø³Ù„")
            print(f"      Ú¯Ø±Ø¯ÛŒ: {props['circularity']:.2f}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        segmenter.save_segmentation_report(results)
        
        # Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ
        print("\nğŸ¥ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ...")
        segmenter.real_time_segmentation()
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")

if __name__ == "__main__":
    main()
                    </code></pre>
                    
                    <!-- Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ -->
                    <div class="mt-3">
                        <div class="card">
                            <div class="card-header d-flex justify-content-between align-items-center">
                                <h6 class="mb-0">Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡</h6>
                                <button class="btn btn-sm btn-outline-secondary" onclick="toggleOutput('image-segmentation-output')">
                                    <i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´
                                </button>
                            </div>
                            <div class="card-body d-none" id="image-segmentation-output">
                                <pre class="bg-light p-3 rounded">
ğŸ¨ Ø³ÛŒØ³ØªÙ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab
==================================================
Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ deeplabv3_resnet101...
âœ… Ù…Ø¯Ù„ deeplabv3_resnet101 Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø´Ø¯
ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ± Ù†Ù…ÙˆÙ†Ù‡...
ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±...
âœ… Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯
â±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: 245.3 Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
ğŸ¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: 8
ğŸ† Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ: background

ğŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø´ÛŒØ§Ø¡:
   person:
      Ù…Ø³Ø§Ø­Øª: 12540 Ù¾ÛŒÚ©Ø³Ù„
      Ù…Ø­ÛŒØ·: 580 Ù¾ÛŒÚ©Ø³Ù„
      Ú¯Ø±Ø¯ÛŒ: 0.47
   car:
      Ù…Ø³Ø§Ø­Øª: 8750 Ù¾ÛŒÚ©Ø³Ù„
      Ù…Ø­ÛŒØ·: 420 Ù¾ÛŒÚ©Ø³Ù„
      Ú¯Ø±Ø¯ÛŒ: 0.62

ğŸ¥ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ...
                                </pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>

    <!-- Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ -->
    <div class="row mt-5">
        <div class="col-md-6">
            <div class="card border-primary">
                <div class="card-body">
                    <h5 class="card-title text-primary">ğŸ“š Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡</h5>
                    <ul class="list-unstyled">
                        <li><strong>OpenCV:</strong> Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆ</li>
                        <li><strong>NumPy:</strong> Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¢Ø±Ø§ÛŒÙ‡â€ŒÙ‡Ø§</li>
                        <li><strong>Matplotlib:</strong> Ø¨ØµØ±ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§</li>
                        <li><strong>TensorFlow/PyTorch:</strong> ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚</li>
                        <li><strong>YOLO/DeepLab:</strong> Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-6">
            <div class="card border-info">
                <div class="card-body">
                    <h5 class="card-title text-info">âš¡ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡</h5>
                    <ul class="list-unstyled">
                        <li>ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡ Ùˆ Ø§Ø´ÛŒØ§Ø¡ Ù¾ÛŒØ´Ø±ÙØªÙ‡</li>
                        <li>ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ</li>
                        <li>Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª Ù‡ÙˆØ´Ù…Ù†Ø¯</li>
                        <li>ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ YOLO</li>
                        <li>Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ DeepLab</li>
                        <li>Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <!-- Ø¨Ø®Ø´ Ø¢Ù…Ø§Ø± Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ -->
    <div class="row mt-5">
        <div class="col-12">
            <div class="card">
                <div class="card-header">
                    <h5 class="mb-0">ğŸ“Š Ø¢Ù…Ø§Ø± Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§</h5>
                </div>
                <div class="card-body">
                    <div class="row">
                        <div class="col-md-3 mb-4">
                            <div class="card text-center">
                                <div class="card-body">
                                    <h2 class="text-primary">5+</h2>
                                    <p class="card-text">Ù¾Ø±ÙˆÚ˜Ù‡ Ú©Ø§Ù…Ù„</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-3 mb-4">
                            <div class="card text-center">
                                <div class="card-body">
                                    <h2 class="text-success">2000+</h2>
                                    <p class="card-text">Ø®Ø· Ú©Ø¯</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-3 mb-4">
                            <div class="card text-center">
                                <div class="card-body">
                                    <h2 class="text-warning">15+</h2>
                                    <p class="card-text">Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…Ø®ØªÙ„Ù</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-3 mb-4">
                            <div class="card text-center">
                                <div class="card-body">
                                    <h2 class="text-info">100%</h2>
                                    <p class="card-text">Ú©Ø¯ Ù…Ù†Ø¨Ø¹ Ø¨Ø§Ø²</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª -->
                    <div class="row mt-4">
                        <div class="col-12">
                            <h6 class="mb-3">Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§</h6>
                            <div class="progress mb-3" style="height: 25px;">
                                <div class="progress-bar bg-success" role="progressbar" style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">ØªØ´Ø®ÛŒØµ Ú†Ù‡Ø±Ù‡: 100%</div>
                            </div>
                            <div class="progress mb-3" style="height: 25px;">
                                <div class="progress-bar bg-info" role="progressbar" style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ: 100%</div>
                            </div>
                            <div class="progress mb-3" style="height: 25px;">
                                <div class="progress-bar bg-warning" role="progressbar" style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">ØªØ´Ø®ÛŒØµ Ø­Ø±Ú©Øª: 100%</div>
                            </div>
                            <div class="progress mb-3" style="height: 25px;">
                                <div class="progress-bar bg-danger" role="progressbar" style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">YOLO: 100%</div>
                            </div>
                            <div class="progress" style="height: 25px;">
                                <div class="progress-bar bg-primary" role="progressbar" style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±: 100%</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- CTA Section -->
    <div class="row mt-5">
        <div class="col-12 text-center">
            <div class="card bg-gradient-primary text-white shadow-lg">
                <div class="card-body py-5">
                    <h3 class="fw-bold mb-3">ğŸ¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ú©Ø§Ù…Ù„</h3>
                    <p class="mb-4">Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ù…Ø®Ø²Ù† GitHub Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯</p>
                    <div class="d-flex flex-wrap justify-content-center gap-3">
                        <a href="{% url 'projects' %}" class="btn btn-light btn-lg">
                            <i class="bi bi-arrow-left me-2"></i>Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§
                        </a>
                        <a href="https://github.com/Amirmohammdkhaki" class="btn btn-outline-light btn-lg" target="_blank">
                            <i class="bi bi-github me-2"></i>Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø¯ Ú©Ø§Ù…Ù„
                        </a>
                        <a href="{% url 'post_list' %}" class="btn btn-outline-light btn-lg">
                            <i class="bi bi-journal-code me-2"></i>Ù…Ù‚Ø§Ù„Ø§Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<style>
    .card {
        border-radius: 1rem;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .card:hover {
        transform: translateY(-5px);
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
    }
    
    .code-container {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 0.5rem;
        font-size: 0.85rem;
        line-height: 1.4;
        position: relative;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø±ÙˆØ² */
    .code-container code {
        color: #d63384;
        font-family: 'Courier New', monospace;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø´Ø¨ */
    [data-theme="dark"] .code-container {
        background: #2d3748;
        border-color: #4a5568;
    }
    
    [data-theme="dark"] .code-container code {
        color: #68d391;
    }
    
    .bg-gradient-primary {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    
    .card-header {
        border-radius: 1rem 1rem 0 0 !important;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ */
    .copy-button {
        position: absolute;
        top: 10px;
        right: 10px;
        z-index: 10;
        opacity: 0.7;
        transition: opacity 0.3s;
    }
    
    .copy-button:hover {
        opacity: 1;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø´Ø¨ */
    [data-theme="dark"] .card {
        background-color: #2d3748;
        color: #e2e8f0;
    }
    
    [data-theme="dark"] .card-header {
        background-color: #1a202c !important;
    }
    
    [data-theme="dark"] .text-muted {
        color: #a0aec0 !important;
    }
    
    [data-theme="dark"] .border-primary {
        border-color: #4299e1 !important;
    }
    
    [data-theme="dark"] .border-info {
        border-color: #63b3ed !important;
    }
    
    [data-theme="dark"] .text-primary {
        color: #63b3ed !important;
    }
    
    [data-theme="dark"] .text-info {
        color: #63b3ed !important;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ */
    .code-section {
        transition: all 0.3s ease;
    }
    
    .code-section:hover {
        transform: translateY(-3px);
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§ */
    .filter-btn {
        transition: all 0.2s ease;
    }
    
    .filter-btn:hover {
        transform: translateY(-2px);
    }
    
    .filter-btn.active {
        background-color: #0d6efd;
        border-color: #0d6efd;
        color: white;
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´ Ø®Ø±ÙˆØ¬ÛŒ */
    .output-section {
        max-height: 200px;
        overflow-y: auto;
    }
    
    /* Ø§Ù†ÛŒÙ…ÛŒØ´Ù†â€ŒÙ‡Ø§ */
    @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
    }
    
    .fade-in {
        animation: fadeIn 0.5s ease-in;
    }
</style>

<script>
// Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø§ÛŒÙ„Ø§ÛŒØª Ú©Ø¯ Ùˆ Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ
document.addEventListener('DOMContentLoaded', function() {
    // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‚Ø§Ø¨Ù„ÛŒØª Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯
    const codeBlocks = document.querySelectorAll('pre code');
    
    codeBlocks.forEach((codeBlock) => {
        const pre = codeBlock.parentElement;
        
        // Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ
        const copyButton = document.createElement('button');
        copyButton.className = 'btn btn-sm btn-outline-secondary copy-button';
        copyButton.innerHTML = '<i class="bi bi-clipboard"></i>';
        copyButton.title = 'Ú©Ù¾ÛŒ Ú©Ø¯';
        
        pre.style.position = 'relative';
        pre.appendChild(copyButton);
        
        copyButton.addEventListener('click', async () => {
            try {
                await navigator.clipboard.writeText(codeBlock.textContent);
                copyButton.innerHTML = '<i class="bi bi-check"></i>';
                copyButton.className = 'btn btn-sm btn-success copy-button';
                
                setTimeout(() => {
                    copyButton.innerHTML = '<i class="bi bi-clipboard"></i>';
                    copyButton.className = 'btn btn-sm btn-outline-secondary copy-button';
                }, 2000);
            } catch (err) {
                console.error('Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù†: ', err);
            }
        });
    });
    
    // Ø§Ù†ÛŒÙ…ÛŒØ´Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§
    const cards = document.querySelectorAll('.card');
    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                entry.target.style.opacity = '1';
                entry.target.style.transform = 'translateY(0)';
            }
        });
    }, { threshold: 0.1 });
    
    cards.forEach(card => {
        card.style.opacity = '0';
        card.style.transform = 'translateY(20px)';
        card.style.transition = 'all 0.6s ease';
        observer.observe(card);
    });
    
    // ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Øª Ø´Ø¨ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ø§Ø³ØªØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨
    function checkDarkMode() {
        const isDarkMode = document.documentElement.getAttribute('data-theme') === 'dark' || 
                          window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (isDarkMode) {
            document.documentElement.setAttribute('data-theme', 'dark');
        } else {
            document.documentElement.removeAttribute('data-theme');
        }
    }
    
    // Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø­Ø§Ù„Øª Ø´Ø¨
    checkDarkMode();
    
    // Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø­Ø§Ù„Øª Ø´Ø¨
    if (window.matchMedia) {
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', checkDarkMode);
    }
    
    // Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ú©Ù…Ù‡ Ø­Ø§Ù„Øª Ø´Ø¨ (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
    const darkModeToggle = document.querySelector('.dark-mode-toggle');
    if (darkModeToggle) {
        darkModeToggle.addEventListener('click', () => {
            const isDarkMode = document.documentElement.getAttribute('data-theme') === 'dark';
            if (isDarkMode) {
                document.documentElement.removeAttribute('data-theme');
            } else {
                document.documentElement.setAttribute('data-theme', 'dark');
            }
        });
    }
    
    // ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯
    const filterButtons = document.querySelectorAll('.filter-btn');
    const codeSections = document.querySelectorAll('.code-section');
    
    filterButtons.forEach(button => {
        button.addEventListener('click', () => {
            // Ø­Ø°Ù Ú©Ù„Ø§Ø³ active Ø§Ø² Ù‡Ù…Ù‡ Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
            filterButtons.forEach(btn => btn.classList.remove('active'));
            // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³ active Ø¨Ù‡ Ø¯Ú©Ù…Ù‡ ÙØ¹Ù„ÛŒ
            button.classList.add('active');
            
            const filter = button.getAttribute('data-filter');
            
            codeSections.forEach(section => {
                if (filter === 'all' || section.getAttribute('data-category').includes(filter)) {
                    section.style.display = 'block';
                    section.classList.add('fade-in');
                } else {
                    section.style.display = 'none';
                }
            });
        });
    });
    
    // Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø¯Ù‡Ø§
    const searchInput = document.getElementById('searchInput');
    if (searchInput) {
        searchInput.addEventListener('input', (e) => {
            const searchTerm = e.target.value.toLowerCase();
            
            codeSections.forEach(section => {
                const title = section.querySelector('.card-header h4').textContent.toLowerCase();
                const description = section.querySelector('.card-body p').textContent.toLowerCase();
                const code = section.querySelector('code').textContent.toLowerCase();
                
                if (title.includes(searchTerm) || description.includes(searchTerm) || code.includes(searchTerm)) {
                    section.style.display = 'block';
                } else {
                    section.style.display = 'none';
                }
            });
        });
    }
});

// ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
function copyCode(codeId) {
    const codeElement = document.getElementById(codeId + '-code');
    const codeText = codeElement.textContent;
    
    navigator.clipboard.writeText(codeText).then(() => {
        // Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ù…ÙˆÙÙ‚ÛŒØª
        showToast('Ú©Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ù¾ÛŒ Ø´Ø¯', 'success');
    }).catch(err => {
        console.error('Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù†: ', err);
        showToast('Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯', 'error');
    });
}

function downloadCode(codeId) {
    const codeElement = document.getElementById(codeId + '-code');
    const codeText = codeElement.textContent;
    const fileName = codeId + '.py';
    
    const blob = new Blob([codeText], { type: 'text/plain' });
    const url = window.URL.createObjectURL(blob);
    
    const a = document.createElement('a');
    a.href = url;
    a.download = fileName;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    window.URL.revokeObjectURL(url);
    
    // Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ù…ÙˆÙÙ‚ÛŒØª
    showToast('Ú©Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯', 'success');
}

function toggleOutput(outputId) {
    const outputElement = document.getElementById(outputId);
    const button = event.target.closest('button');
    
    if (outputElement.classList.contains('d-none')) {
        outputElement.classList.remove('d-none');
        button.innerHTML = '<i class="bi bi-eye-slash"></i> Ù…Ø®ÙÛŒ';
    } else {
        outputElement.classList.add('d-none');
        button.innerHTML = '<i class="bi bi-eye"></i> Ù†Ù…Ø§ÛŒØ´';
    }
}

function showToast(message, type = 'info') {
    // Ø§ÛŒØ¬Ø§Ø¯ toast
    const toast = document.createElement('div');
    toast.className = `toast align-items-center text-white bg-${type === 'success' ? 'success' : type === 'error' ? 'danger' : 'primary'} border-0`;
    toast.setAttribute('role', 'alert');
    toast.setAttribute('aria-live', 'assertive');
    toast.setAttribute('aria-atomic', 'true');
    
    toast.innerHTML = `
        <div class="d-flex">
            <div class="toast-body">
                ${message}
            </div>
            <button type="button" class="btn-close btn-close-white me-2 m-auto" data-bs-dismiss="toast" aria-label="Close"></button>
        </div>
    `;
    
    // Ø§ÛŒØ¬Ø§Ø¯ container Ø¨Ø±Ø§ÛŒ toast Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    let toastContainer = document.querySelector('.toast-container');
    if (!toastContainer) {
        toastContainer = document.createElement('div');
        toastContainer.className = 'toast-container position-fixed bottom-0 end-0 p-3';
        document.body.appendChild(toastContainer);
    }
    
    // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† toast Ø¨Ù‡ container
    toastContainer.appendChild(toast);
    
    // Ù†Ù…Ø§ÛŒØ´ toast
    const bsToast = new bootstrap.Toast(toast);
    bsToast.show();
    
    // Ø­Ø°Ù toast Ù¾Ø³ Ø§Ø² Ù…Ø®ÙÛŒ Ø´Ø¯Ù†
    toast.addEventListener('hidden.bs.toast', () => {
        toast.remove();
    });
}
</script>
{% endblock %}